%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Dmitry Sorokin at 2020-10-23 21:29:08 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@misc{steamSecretive,
	Author = {{PC Games}},
	Date-Added = {2020-10-23 16:25:18 -0400},
	Date-Modified = {2020-10-23 16:26:37 -0400},
	Howpublished = {PSGames},
	Keywords = {News},
	Title = {Valve shuts down another way to estimate Steam sales, and indie devs aren't happy},
	Url = {https://www.pcgamesn.com/steam-sales-estimates},
	Year = {2018},
	Bdsk-Url-1 = {https://www.pcgamesn.com/steam-sales-estimates}}

@article{DownsRocke94,
	Abstract = {The problem of ensuring that chief executives act in accordance with the wishes of their constituency is particularly acute in the area of foreign intervention where the head of state can be expected to possess substantial information advantages. This paper presents a formal analysis of strategies that can be used to deter overly passive and overly aggressive executives and a discussion of their side effects. The typically large amount of uncertainty means that the constituency must base its decision to retain an executive on the outcome of a conflict and not on its apparent ex ante advisability. This uncertainty imposes a cost on the constituency, who may remove an effective, "innocent" executive unnecessarily, and it also imposes a cost on the well-meaning executive, who may be removed from office after making the best possible decision in a difficult case. The mechanism necessary to deter executive adventurism also causes the paradoxical "gambling for resurrection" effect, in which an unsuccessful war that a well-informed principal would terminate is continued because cessation would, given the current state of the world, cause the agent to be removed from office.},
	Author = {George W. Downs and David M. Rocke},
	Date-Added = {2020-10-23 15:44:44 -0400},
	Date-Modified = {2020-10-23 15:45:03 -0400},
	Journal = {American Journal of Political Science},
	Number = {2},
	Pages = {362--380},
	Title = {Conflict, Agency, and Gambling for Resurrection: The Principal-Agent Problem Goes to War},
	Volume = {38},
	Year = {1994},
	Bdsk-Url-1 = {http://www.jstor.org/stable/2111408}}

@article{Dholakia11,
	Abstract = {We examine performance of daily deals run through five major sites in 23 US markets. In a survey-based study of 324 businesses that conducted a daily deal promotion between August 2009 and March 2011, 55.5% of businesses reported making money, 26.6% lost money and 17.9% broke even on their promotions. Although close to 80% of deal users were new customers, significantly fewer users spent beyond the deal's value or returned to purchase at full price. 48.1% of businesses indicated they would run another daily deal promotion, 19.8% said they would not, and 32.1% said they were uncertain. We also examined drivers of deal profitability, the loyalty of merchants to a daily deal site, and how spending on daily deals has affected spending of businesses on other marketing programs. Overall, our findings lead us to conclude that there are relatively few points of differentiation between the daily deal sites, making it harder for any one site to stand out from the others. Our findings also uncovered a number of red flags regarding the industry as a whole: (1) the relatively low percentages of deal users spending beyond the deal value (35.9%) and returning for a full-price purchase (19.9%) are symptomatic of a structural weakness in the daily deal business model, (2) less than half of the businesses indicated enthusiasm about running another daily deal in the future, (3) fully 72.8% indicated openness to considering a different daily deal site, and (4) only 35.9% of restaurants/ bars and 41.5% of salons and spas that had run a daily deal asserted they would run another such promotion in the future. All of these findings point to the same conclusion: Over the next few years, it is likely that daily deal sites will have to settle for lower shares of revenues from businesses compared to their current levels, and it will be harder and more expensive for them to find viable candidates to fill their pipelines of daily deals. },
	Author = {Utpal M. Dholakia},
	Date-Added = {2020-10-23 15:35:40 -0400},
	Date-Modified = {2020-10-23 21:27:57 -0400},
	Journal = {{SSRN} Electronic Journal},
	Title = {How Businesses Fare with Daily Deals: A Multi-Site Analysis of Groupon, Livingsocial, Opentable, Travelzoo, and BuyWithMe Promotions},
	Year = 2011,
	Bdsk-Url-1 = {https://doi.org/10.2139%2Fssrn.1863466}}

@article{AperjisJohari10,
	Abstract = {A seller in an online marketplace with an effective reputation mechanism should expect that dishonest behavior results in higher payments now whereas honest behavior results in a better reputation---and thus higher payments---in the future. We study the Window Aggregation Mechanism, a widely used class of mechanisms that shows the average value of the seller's ratings within some fixed window of past transactions. We suggest approaches for choosing the window size that maximizes the range of parameters for which it is optimal for the seller to be truthful. We show that mechanisms that use information from a larger number of past transactions tend to provide incentives for patient sellers to be more truthful but for higher-quality sellers to be less truthful.},
	Author = {Christina Aperjis and Ramesh Johari},
	Date-Added = {2020-10-22 16:03:16 -0400},
	Date-Modified = {2020-10-22 16:04:06 -0400},
	Journal = {Management Science},
	Keywords = {Theory, Review Systems},
	Month = {May},
	Number = {5},
	Pages = {864--880},
	Title = {Optimal Windows for Aggregating Ratings in Electronic Marketplaces},
	Volume = {56},
	Year = 2010,
	Bdsk-Url-1 = {https://doi.org/10.1287/mnsc.1090.1145}}

@article{Kovbasyuk18,
	Author = {Kovbasyuk, Sergey, and Spagnolo, Giancarlo},
	Date-Added = {2020-10-22 15:49:57 -0400},
	Date-Modified = {2020-10-22 15:51:16 -0400},
	Journal = {Working Paper},
	Keywords = {Theory, Learning, Review Systems},
	Title = {Memory and Markets},
	Year = {2018}}

@article{cheHorner18,
	Author = {Che, Yeon-Koo and H{\"o}rner, Johannes},
	Date-Added = {2020-10-22 15:48:45 -0400},
	Date-Modified = {2020-10-22 15:49:12 -0400},
	Journal = {The Quarterly Journal of Economics},
	Keywords = {Theory, Review Systems, Learning},
	Number = {2},
	Pages = {871--925},
	Publisher = {Oxford University Press},
	Title = {Recommender systems as mechanisms for social learning},
	Volume = {133},
	Year = {2018}}

@article{Vellodi20,
	Abstract = {I study the impact of consumer reviews on the incentives for firms to participate in the marketthrough the lens of ratings design. Firms produce goods of heterogeneous, unknown quality that isgradually revealed via consumer reviews, and face both entry and exit decisions. A platform combinespast reviews to construct firm-specific ratings that help guide consumer search. When the platformintegrates all reviews into ratings -- full transparency -- consumers form queues at the highest-ratedfirms. This demand profile induces an S-shaped continuation value for firms as a function of ratings.Whereas firms thus prefer more feedback when struggling and less feedback when established, equilib-rium induces precisely the reverse allocation. The main insight of the paper is that suppressing thereviews of highly-rated firms makes the task of climbing the ratings ladder less arduous, thus stim-ulating participation and consumer welfare. This insight is robust to extensions that allow for pricesetting, costly effort and heterogeneous consumer preferences.},
	Author = {Vellodi, Nikhil},
	Date-Added = {2020-10-22 14:33:39 -0400},
	Date-Modified = {2020-10-22 15:44:49 -0400},
	Journal = {Working Paper},
	Keywords = {Theory, Review Systems},
	Title = {Ratings design and barriers to entry},
	Year = {2020},
	Bdsk-File-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxAiLi4vLi4vLi4vRG93bmxvYWRzL1ZlbGxvZGkyMDE4LnBkZk8RAUwAAAAAAUwAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////w9WZWxsb2RpMjAxOC5wZGYAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAMAAgAACiBjdQAAAAAAAAAAAAAAAAAJRG93bmxvYWRzAAACACgvOlVzZXJzOmRtaXRyeTpEb3dubG9hZHM6VmVsbG9kaTIwMTgucGRmAA4AIAAPAFYAZQBsAGwAbwBkAGkAMgAwADEAOAAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAJlVzZXJzL2RtaXRyeS9Eb3dubG9hZHMvVmVsbG9kaTIwMTgucGRmABMAAS8AABUAAgAN//8AAAAIAA0AGgAkAEkAAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAABmQ==}}

@article{texreg,
	Author = {Leifeld, Philip},
	Journal = {Journal of Statistical Software},
	Number = {8},
	Pages = {1--24},
	Title = {{texreg}: Conversion of Statistical Model Output in {R} to {\LaTeX} and {HTML} Tables},
	Volume = {55},
	Year = {2013}}

@manual{here,
	Author = {M{\"u}ller, Kirill},
	Note = {R package version 0.1},
	Title = {here: A Simpler Way to Find Your Files},
	Url = {https://CRAN.R-project.org/package=here},
	Year = {2017},
	Bdsk-Url-1 = {https://CRAN.R-project.org/package=here}}

@manual{dataframe,
	Author = {Dowle, Matt and Srinivasan, Arun},
	Note = {R package version 1.13.0},
	Title = {data.table: Extension of `data.frame`},
	Url = {https://CRAN.R-project.org/package=data.table},
	Year = {2020},
	Bdsk-Url-1 = {https://CRAN.R-project.org/package=data.table}}

@article{plm,
	Author = {Croissant, Yves and Millo, Giovanni},
	Journal = {Journal of Statistical Software},
	Number = {2},
	Pages = {1--43},
	Title = {Panel Data Econometrics in {R}: The {plm} Package},
	Volume = {27},
	Year = {2008}}

@article{stargazer,
	Author = {Hlavac, Marek},
	Date-Added = {2020-10-13 11:37:43 -0400},
	Date-Modified = {2020-10-13 11:37:52 -0400},
	Journal = {URL: http://CRAN. R-project. org/package= stargazer},
	Title = {stargazer: LaTeX code and ASCII text for well-formatted regression and summary statistics tables},
	Year = {2013}}

@periodical{steamPurge,
	Author = {Batchelor, James},
	Date-Added = {2020-10-11 12:27:34 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Journal = {gamesindustry.biz},
	Keywords = {News},
	Title = {Steam removes hundreds of games after publishers abuse Steamworks},
	Year = {2019},
	Bdsk-Url-1 = {https://www.gamesindustry.biz/articles/2019-11-26-steam-removes-hundreds-of-games-after-publishers-abuse-steamworks}}

@book{Tirole17,
	Author = {Tirole, Jean},
	Date-Added = {2020-10-09 13:21:20 -0400},
	Date-Modified = {2020-10-09 13:28:36 -0400},
	Publisher = {Princeton University Press},
	Title = {Economics for the common good},
	Year = {2017}}

@article{IfrachEtAl19,
	Author = {Bar Ifrach and Costis Maglaras and Marco Scarsini and Anna Zseleva},
	Date-Added = {2020-09-10 16:51:07 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/opre.2019.1861},
	Journal = {Operations Research},
	Keywords = {Selection, Theory},
	Month = {September},
	Number = {5},
	Pages = {1209--1221},
	Title = {Bayesian Social Learning from Consumer Reviews},
	Volume = {67},
	Year = 2019,
	Bdsk-Url-1 = {https://doi.org/10.1287%2Fopre.2019.1861},
	Bdsk-Url-2 = {https://doi.org/10.1287/opre.2019.1861},
	Bdsk-Url-3 = {https://doi.org/10.1287%5C%2Fopre.2019.1861}}

@unpublished{Hansen20,
	Author = {Hansen, Bruce H.},
	Date-Added = {2020-08-07 14:03:35 -0400},
	Date-Modified = {2020-10-11 12:29:23 -0400},
	Note = {Manuscript},
	Title = {Econometrics},
	Year = {2020}}

@article{LinEtAl18,
	Abstract = {The steadily increasing popularity of computer games has led to the rise of a multi-billion dollar industry. Due to the scale of the computer game industry, developing a successful game is challenging. In addition, prior studies show that gamers are extremely hard to please, making the quality of games an important issue. Most online game stores allow users to review a game that they bought. Such reviews can make or break a game, as other potential buyers often base their purchasing decisions on the reviews of a game. Hence, studying game reviews can help game developers better understand user concerns, and further improve the user-perceived quality of games. In this paper, we perform an empirical study of the reviews of 6,224 games on the Steam platform, one of the most popular digital game delivery platforms, to better understand if game reviews share similar characteristics with mobile app reviews, and thereby understand whether the conclusions and tools from mobile app review studies can be leveraged by game developers. In addition, new insights from game reviews could possibly open up new research directions for research of mobile app reviews. We first conduct a preliminary study to understand the number of game reviews and the complexity to read through them. In addition, we study the relation between several game-specific characteristics and the fluctuations of the number of reviews that are received on a daily basis. We then focus on the useful information that can be acquired from reviews by studying the major concerns that users express in their reviews, and the amount of play time before players post a review. We find that game reviews are different from mobile app reviews along several aspects. Additionally , the number of playing hours before posting a review is a unique and helpful attribute for developers that is not found in mobile app reviews. Future longitudinal studies should be conducted to help developers and researchers leverage this information. Although negative reviews contain more valuable information about the negative aspects of the game, such as mentioned complaints and bug reports, developers and researchers should also not ignore the potentially useful information in positive reviews. Our study on game reviews serves as a starting point for other game review researchers, and suggests that prior studies on mobile app reviews may need to be revisited.},
	Annote = {This is one of the papers by this guy who descrbed everything on Steam.},
	Author = {Lin, Dayi and Bezemer, Cor-Paul and Zou, Ying and Hassan, Ahmed E.},
	Date-Added = {2020-07-31 19:44:31 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1007/s10664-018-9627-4},
	Issn-Hide = {1573-7616},
	Journal = {Empirical Software Engineering},
	Keywords = {Steam, Video Games},
	Month = {Jun},
	Number = {1},
	Pages = {170--207},
	Publisher = {Springer Science and Business Media LLC},
	Title = {An empirical study of game reviews on the Steam platform},
	Volume = {24},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s10664-018-9627-4}}

@article{DaiEtAl17,
	Abstract = {Because consumer reviews leverage the wisdom of the crowd, the way in which they are aggregated is a central decision faced by platforms. We explore this ``rating aggregation problem'' and offer a structural approach to solving it, allowing for (1) reviewers to vary in stringency and accuracy, (2) reviewers to be influenced by existing reviews, and (3) product quality to change over time. Applying this to restaurant reviews from Yelp.com, we construct an adjusted average rating and show that even a simple algorithm can lead to large information efficiency gains relative to the arithmetic average.},
	Annote = {This paper studies a learning setting in which the quality of the restaurant follows a random walk. A consumer gets a signal of the quality, she sees all reviews left by previous consumers, and she leaves a review that balances some social desires with informativeness. Because of the normality assumption, consumers can use Kalman Filter to efficiently estimate the current quality.

The paper takes this model to data, in which consumers rank restaurants on the scale 1-5 (not very normal). The estimated model allows to back out real time quality of restaurants, which the paper suggests to be a better summary statistics than the average of reviews.

The issue with the paper is that the model doesn't allow to study the counterfactual of having a different review system, because in the model consumers observe all reviews and don't care about summary stats. },
	Author = {Dai, Weijia and Jin, Ginger and Lee, Jungmin and Luca, Michael},
	Date-Added = {2020-07-31 16:37:55 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1007/s11129-017-9194-9},
	Issn-Hide = {1573-711X},
	Journal = {Quantitative Marketing and Economics},
	Keywords = {Restaurants, Learning, Review Systems, Design},
	Month = {Dec},
	Number = {3},
	Pages = {289--339},
	Publisher = {Springer Science and Business Media LLC},
	Title = {Aggregation of consumer ratings: an application to Yelp.com},
	Volume = {16},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s11129-017-9194-9}}

@article{AcemogluEtAl19,
	Abstract = {This paper develops a model of Bayesian learning from online reviews and investigates
the conditions for asymptotic learning of the quality of a product and the speed of learning
under different rating systems. A rating system provides information about reviews left by
previous customers. Potential customers observe the ratings of a product, and conditional on
their ex ante valuation, decide whether or not to purchase it. After purchase, they leave a
review based on their realized utility. We study learning dynamics under two classes of rating
systems: full history, where customers see the full history of reviews, and summary statistics,
where the platform reports some summary statistics of past reviews. In both cases, learning
dynamics are complicated by a selection effect --- the types of users who purchase the good and
thus their overall satisfaction and reviews depend on the information that they have available
at the time of their purchase. We provide conditions for asymptotic learning and characterize
and compare its speed under full history and summary statistics. We also show that providing
more information does not always lead to faster learning, but strictly finer rating systems do.},
	Author = {Acemoglu, Daron and Makhdoumi, Ali and Malekian, Azarakhsh and Ozdaglar, Asu},
	Date-Added = {2020-07-31 15:43:48 -0400},
	Date-Modified = {2020-07-31 15:46:11 -0400},
	Keywords = {Selection, Theory, Learning},
	Month = {June},
	Title = {Learning From Reviews: The Selection Effect and the Speed of Learning},
	Year = {2019},
	Bdsk-Url-1 = {https://economics.mit.edu/files/17178}}

@misc{steamDiscounting,
	Author = {{Steamworks Documentation}},
	Date-Added = {2020-07-20 17:27:00 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Keywords = {News},
	Title = {Discounting},
	Year = {2020},
	Bdsk-Url-1 = {https://partner.steamgames.com/doc/marketing/discounts}}

@misc{steamVisibility2,
	Author = {{Steamworks Documentation}},
	Date-Added = {2020-07-19 16:02:03 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Keywords = {News},
	Month = {May},
	Title = {Positive "Review Bombs"},
	Year = {2019},
	Bdsk-Url-1 = {https://steamcommunity.com/games/593110/announcements/detail/1621770561051427036}}

@misc{steamVisibility,
	Author = {{Steamworks Documentation}},
	Date-Added = {2020-07-19 15:17:05 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Keywords = {News},
	Month = {July},
	Title = {Visibility on Steam},
	Year = {2020},
	Bdsk-Url-1 = {https://partner.steamgames.com/doc/marketing/visibility}}

@misc{steamNPD,
	Author = {{Entertainment Software Association}},
	Date-Added = {2020-07-16 15:54:39 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Journal = {Statista},
	Keywords = {Data},
	Month = {May},
	Title = {Breakdown of U.S. computer and video game sales from 2009 to 2017, by delivery format},
	Year = {2019},
	Bdsk-Url-1 = {https://www.statista.com/statistics/190225/digital-and-physical-game-sales-in-the-us-since-2009/}}

@periodical{videoGamesMarket,
	Author = {Mike Minotti},
	Date-Added = {2020-07-16 15:53:46 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Journal = {Venturebeat},
	Keywords = {News, Data},
	Title = {NPD: U.S. game sales hit a record \$43.4 billion in 2018},
	Year = {2019},
	Bdsk-Url-1 = {https://venturebeat.com/2019/01/22/npd-u-s-game-sales-hit-a-record-43-4-billion-in-2018/}}

@misc{psgames,
	Author = {{PC Games}},
	Date-Added = {2020-07-16 15:49:23 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Howpublished = {PSGames},
	Keywords = {News, Data},
	Title = {With \$4.3 billion in sales, 2017 was Steam's biggest year yet},
	Year = {2017},
	Bdsk-Url-1 = {https://www.pcgamesn.com/steam-revenue-2017}}

@periodical{steamBloomberg,
	Author = {Edwards Cliff},
	Date-Added = {2020-07-16 15:43:23 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Journal = {Bloomberg},
	Keywords = {Data, News},
	Title = {Valve Lines Up Console Partners in Challenge to Microsoft, Sony},
	Year = {2013},
	Bdsk-Url-1 = {https://web.archive.org/web/20140914033025/http://www.bloomberg.com/news/2013-11-04/valve-lines-up-console-partners-in-challenge-to-microsoft-sony.html}}

@article{CabralLi15,
	Abstract = {We run a series of controlled field experiments on eBay where buyers are rewarded for providing feedback. Our results provide little support for the hypothesis of buyers' rational economic behavior: the likelihood of feedback barely increases as we increase feedback rebate values; also, the speed of feedback, bid levels, and the number of bids are all insensitive to rebate values. By contrast, we find evidence consistent with reciprocal buyer behavior. Lower transaction quality leads to a higher probability of negative feedback as well as a speeding up of such negative feedback. However, when transaction quality is low (as measured by slow shipping), offering a rebate significantly decreases the likelihood of negative feedback. All in all, our results are consistent with the hypothesis that buyers reciprocate the sellers' ``good deeds'' (feedback rebate, high transaction quality) with more frequent and more favorable feedback. As a result, sellers can ``buy'' feedback, but such feedback is likely to be biased.},
	Annote = {Finds evidence that consumers reciprocate in giving reviews, which is a theory different from selection or matching quality.},
	Author = {Cabral, Lu\'is and Li, Lingfang (Ivy)},
	Date-Added = {2020-07-15 15:00:32 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/mnsc.2014.2074},
	Issn-Hide = {1526-5501},
	Journal = {Management Science},
	Keywords = {Experiment, Biases, Price promotion, Responses},
	Month = {Sep},
	Number = {9},
	Pages = {2052--2063},
	Publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	Title = {A Dollar for Your Thoughts: Feedback-Conditional Rebates on eBay},
	Volume = {61},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/mnsc.2014.2074}}

@article{Zegners17,
	Abstract = {New sellers who enter online marketplaces have to build up an online reputation in order to be successful. I examine how sellers use "free'' (offering products at a zero price) as a dynamic pricing strategy to encourage consumers to experience their products, thereby increasing the number of online reviews and contributing towards building up an online reputation. Using data from an online self-publishing platform, I show that the same e-book is more likely to be offered as free content when it has gathered fewer previous reviews and receives more reviews when it is offered as free content. However, the same e-book receives worse reviews when it is offered for free. This negative impact of "free'' on an e-book's online reputation is driven by a selection effect, whereby more consumers with a lower preference for a particular e-book provide reviews when the e-book is offered as free content. These results imply that although "free'' can be effective to build up an online reputation more quickly, sellers face the trade-off that the earned reputation will be worse.},
	Annote = {Shows that books are sold for free to gain some visibility. This does lead to new reviews (+1-5%), but lowers the raiting by  0.05 to 0.06 stars (out of 5 possible stars), which is 6% of the s.d., and the text of reviews for free e-books is shorter and written in a worse sentiment. He finds that selection of different customers into the reviewers pool explains the majority of the effect.},
	Author = {Zegners, Dainis},
	Date-Added = {2020-07-14 18:12:32 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.2139/ssrn.2753635},
	Issn-Hide = {1556-5068},
	Journal = {SSRN Electronic Journal},
	Keywords = {Price promotion, Reviews, Books, Responses, Selection},
	Publisher = {Elsevier BV},
	Title = {Building an Online Reputation with Free Content: Evidence from the E-Book Market},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.2139/ssrn.2753635}}

@article{Li16,
	Abstract = {It is by now almost accepted as a stylized fact that offering deal promotion (such as via Groupon or LivingSocial) deteriorates local merchants' online reputations (e.g., the average of Yelp review ratings). However, in this paper we show that the stylized fact is not true in certain circumstances. We theorize that the valence and volume of prior reviews can play an important moderating role in the effect of deal promotion. Empirically, we show that restaurants with a relatively low prior average rating and a relatively small review volume have improved their online reputations by offering Groupon promotion. The proportion of such restaurants is substantial. The findings are robust to multiple identification strategies and econometric specifications. The results underscore the substantial heterogeneity in the effect of deal promotion on local merchants' online reputations. Merchants need to understand the moderating role of prior reviews (e.g., the valence and volume of prior reviews) and design appropriate strategies to maximize the returns from offering deal promotion.},
	Annote = {Compared to (Byers, Mitzenmacher, and Zervas 12), this guys attempt to study the causal effect of a groupon promotion on reviews on Yelp. They construct a synthetic control group and run a DID. They confirm the average negative effect of a Groupon promotion on reviews, but find that smaller review score and number of reviews moderate the effect, and that such businesses with worse/fewer reviews can actually benefit from the promotion.},
	Author = {Li, Xitong},
	Date-Added = {2020-07-14 16:50:59 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1080/07421222.2016.1172450},
	Issn-Hide = {1557-928X},
	Journal = {Journal of Management Information Systems},
	Keywords = {Price promotion, Restaurants},
	Month = {Jan},
	Number = {1},
	Pages = {171--201},
	Publisher = {Informa UK Limited},
	Title = {Could Deal Promotion Improve Merchants' Online Reputations? The Moderating Role of Prior Reviews},
	Volume = {33},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1080/07421222.2016.1172450}}

@article{LuEtAl13,
	Abstract = {The value of promotional marketing and word-of-mouth (WOM) is well recognized, but few studies have compared the effects of these two types of information in online settings. This research examines the effect of marketing efforts and online WOM on product sales by measuring the effects of online coupons, sponsored keyword search, and online reviews. It aims to understand the relationship between firms' promotional marketing and WOM in the context of a third party review platform. Using a three-year panel data set from one of the biggest restaurant review websites in China, the study finds that both online promotional marketing and reviews have a significant impact on product sales, which suggests promotional marketing on third party review platforms is still an effective marketing tool. This research further explores the interaction effects between WOM and promotional marketing when these two types of information coexist. The results demonstrate a substitute relationship between the WOM volume and coupon offerings, but a complementary relationship between WOM volume and keyword advertising. },
	Annote = {This paper regresses log(sales) on reviews, discounts, and their interactions.},
	Author = {Lu, Xianghua and Ba, Sulin and Huang, Lihua and Feng, Yue},
	Date-Added = {2020-07-14 16:40:15 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/isre.1120.0454},
	Eprint = {https://pubsonline.informs.org/doi/pdf/10.1287/isre.1120.0454},
	Journal = {Information Systems Research},
	Keywords = {Restaurants, Sales, Price promotion},
	Number = {3},
	Pages = {596-612},
	Title = {Promotional Marketing or Word-of-Mouth? Evidence from Online Restaurant Reviews},
	Volume = {24},
	Year = {2013},
	Bdsk-Url-1 = {https://pubsonline.informs.org/doi/abs/10.1287/isre.1120.0454},
	Bdsk-Url-2 = {https://doi.org/10.1287/isre.1120.0454}}

@article{ByersEtAl12,
	Abstract = {Daily deals sites such as Groupon offer deeply discounted goods and services to tens of millions of customers through geographically targeted daily e-mail marketing campaigns. In our prior work we observed that a negative side effect for merchants selling Groupons is that, on average, their Yelp ratings decline significantly. However, this previous work was primarily observational, rather than explanatory. In this work, we rigorously consider and evaluate various hypotheses about underlying consumer and merchant behavior in order to understand this phenomenon, which we dub the Groupon effect. We use statistical analysis and mathematical modeling, leveraging a dataset we collected spanning tens of thousands of daily deals and over 7 million Yelp reviews. We investigate hypotheses such as whether Groupon subscribers are more critical than their peers, whether Groupon users are experimenting with services and merchants outside their usual sphere, or whether some fraction of Groupon merchants provide significantly worse service to customers using Groupons. We suggest an additional novel hypothesis: reviews from Groupon users are lower on average because such reviews correspond to real, unbiased customers, while the body of reviews on Yelp contain some fraction of reviews from biased or even potentially fake sources. Although our focus is quite specific, our work provides broader insights into both consumer and merchant behavior within the daily deals marketplace.},
	Annote = { A fairly detailed paper. They start by plotting a convincing dip in the Yelp score right after a Groupon promotion, and calculating that ``Groupon reviews have a mean score of 3.27 stars, while non-Groupon reviews have a mean of 3.73 stars''.

They consider the following explanations:

1) Reviews just go down over time (no way that explains the jump)

2) Groupon Customers are more critical. They find some evidence of that, with grubhub customers leaving a bit more critical reviews on average, but being significantly more moderate (fewer 1 and 5 stars). 

Groupon users are different. More importantly, Groupon users have more friends/fans and stuff on Yelp, and write more reviews. I think their big problem is selection into the sample, because they only classify you as ``Groupon'' if you mention the word in the reviews. Thus, they select for the people who write lengthier and higher quality reviews. The effect of Groupon is hard to pin down here. 

They also find evidence of experimentation induced by Groupon, which is the selection effect I want to emphasize.

3) Businesses that offer Groupon discounts are bad, or fail to cope with the influx of customers. They claim they have limited evidence of this, but I did not get it, looks really weird.

4) The non-Groupon reviews are inflated. This is a nice one, they are saying that Groupon reviews could actually be more critical because they are more representative. They use 200 random reviewers who left a shit-ton of reviews (how random is that?) to show that Groupon reviews are less likely to be flagged by Yelp as fake, and thus to be excluded from the restaurant's review list (but not from the user's review list). 

* * *
Takeaways for me. First, obviously, my findings go in the opposite direction. About their hypotheses, I can not say anything about individual users. However, there are some key differences in settings. There is no reason why Steam users buying on a discount would more likely to be influencers, or something like that, but who knows. At least they are buying the game on the same platform as non-discount users. Importantly, I am not subject to the selection issue that they are subject to, when they only use reviews mentioning ``Groupon'' as ``Groupon'' reviews. 

The bad business concern is something I should definitely think about more closely, but, (A) I control for the score in the regs, and (B), my consumers actualy leave better reviews on discounts, so the effect goes in the opposite direction. Maybe, I should show that better businesses give fewer discounts, just to make sure. },
	Author = {Byers, John W. and Mitzenmacher, Michael and Zervas, Georgios},
	Date-Added = {2020-07-14 15:28:44 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1145/2229012.2229034},
	Isbn = {9781450314152},
	Journal = {Proceedings of the 13th ACM Conference on Electronic Commerce - EC '12},
	Keywords = {Price promotion, Grubhub, Restaurants},
	Publisher = {ACM Press},
	Title = {The groupon effect on yelp ratings},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2229012.2229034}}

@article{ZhuEtAl19,
	Abstract = {Online consumer reviews (OCRs) are valuable to consumers and sellers. Online price promotion is commonly used by local merchants to increase sales. However, knowledge of the differences in OCRs between consumers who received a discount and regular consumers is limited. This study investigates the effects of price discounts on restaurant OCRs by comparing the review rating and open-ended contents of OCRs from consumers who received a discount and regular consumers. The results show that the review rating is higher from consumers who received a discount, whereas the word count, image count, and diversity of review contents are higher from regular consumers. Regular consumers are more likely to mention product quality, environmental quality, service quality, geographic location, purchasing process, recommendation expression, and loyalty expression in OCRs, and there is no significant difference in the dimensions of price, cognitive attitude, and emotional attitude between the two groups.},
	Annote = {The paper has an insane literature review. The content is simply testing for differences in means across discount groups.},
	Author = {Dong Hong Zhu and Zhi Jie Zhang and Ya Ping Chang and Shichang Liang},
	Date-Added = {2020-07-13 16:25:06 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {https://doi.org/10.1016/j.ijhm.2018.06.028},
	Issn-Hide = {0278-4319},
	Journal = {International Journal of Hospitality Management},
	Keywords = {Price promotion, Restaurants},
	Pages = {178 - 186},
	Title = {Good discounts earn good reviews in return? Effects of price promotion on online restaurant reviews},
	Volume = {77},
	Year = {2019},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S027843191731023X},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ijhm.2018.06.028}}

@article{Ishihara_2019,
	Abstract = {For information/digital products, the used goods market has been viewed as a threat by producers. However, it is not clear whether this view is justified because the used goods market also provides owners with an opportunity to sell their products. To investigate the impact of the used goods market on new goods sales, we collect a unique data set from the Japanese video game market. On the basis of the data, we develop and estimate a new dynamic structural model of consumers' buying and selling decisions. The estimation results show that potential buyers' consumption value from a game deteriorates by 50% from the release week to the second week, and game owners' consumption value deteriorates by 23%--58% after the first week of ownership, and the rate depends on game characteristics. Examination of the cross-price elasticities suggests that the elasticities tend to be high especially when the used-game inventory at retailers is low, but they quickly decrease as the inventory is accumulated. Using the estimates, we quantify the impact of eliminating the used game market on publishers' profits and consumer welfare. We find that holding the new-copy price at the observed level, this policy would increase publishers' profits by 7.3% but reduce the consumer surplus by 0.9%, resulting in an overall decrease in social surplus by 0.3%. However, if firms adjust prices optimally, it would increase the profits by 26.8% and also increase the consumer surplus by 1.4% owing to lower new game's prices. Overall, the social surplus increases by 2.7%.},
	Author = {Ishihara, Masakazu and Ching, Andrew T.},
	Date-Added = {2020-07-04 11:31:06 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/mksc.2018.1142},
	Issn-Hide = {1526-548X},
	Journal = {Marketing Science},
	Keywords = {Video Games},
	Month = {May},
	Number = {3},
	Pages = {392--416},
	Publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	Title = {Dynamic Demand for New and Used Durable Goods Without Physical Depreciation: The Case of Japanese Video Games},
	Volume = {38},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/mksc.2018.1142}}

@article{Luca16,
	Author = {Luca, Michael},
	Date-Added = {2020-07-02 15:54:10 -0400},
	Date-Modified = {2020-07-20 16:41:18 -0400},
	Journal = {Working Paper},
	Keywords = {Causal, Sales},
	Title = {Reviews, Reputation and Revenue: The Case of Yelp.Com},
	Year = 2016}

@article{AndersonMagruder12,
	Abstract = {Internet review forums increasingly supplement expert opinion and social networks in informing consumers about product quality. However, limited empirical evidence links digital word-of-mouth to purchasing decisions. We implement a regression discontinuity design to estimate the effect of positive Yelp.com ratings on restaurant reservation availability. An extra half-star rating causes restaurants to sell out 19 percentage points (49\%) more frequently, with larger impacts when alternate information is more scarce. These returns suggest that restaurateurs face incentives to leave fake reviews but a rich set of robustness checks confirm that restaurants do not manipulate ratings in a confounding, discontinuous manner.},
	Author = {Anderson, Michael and Magruder, Jeremy},
	Date-Added = {2020-07-02 15:47:14 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1111/j.1468-0297.2012.02512.x},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1468-0297.2012.02512.x},
	Journal = {The Economic Journal},
	Keywords = {Sales, Causal, Restaurants},
	Number = {563},
	Pages = {957-989},
	Title = {Learning from the Crowd: Regression Discontinuity Estimates of the Effects of an Online Review Database},
	Volume = {122},
	Year = {2012},
	Bdsk-Url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0297.2012.02512.x},
	Bdsk-Url-2 = {https://doi.org/10.1111/j.1468-0297.2012.02512.x}}

@techreport{ReimersWaldfogel20,
	Abstract = {Digitization has led to product proliferation, straining traditional institutions for product discovery; but digitization has also spawned crowd-based rating systems. We compare the relative impacts of professional critics and crowd-based Amazon star ratings on consumer welfare in book publishing. We assemble data on daily Amazon sales ranks, star ratings, and prices for thousands of books in 2018, along with information on their professional reviews in several major outlets. Using various fixed effects and discontinuity-based empirical strategies, we estimate that a New York Times review raises estimated sales by 78 percent during the first five days following a review; and the elasticity of sales with respect to an Amazon star is about 0.75. We use these causal estimates to calibrate structural models of demand for measuring the welfare impact of pre-purchase information in a way that respects the distinction between ex ante and ex post utility. The aggregate effect of star ratings on consumer surplus is roughly 15 times the effect of traditional review outlets. Crowd-based information now accounts for the vast majority of pre-purchase information, but the absolute effects of professional reviews have not declined over time.},
	Annote = {Regresses log(sales rank) on its lag, log Amazon Star score, log price, log reviews, interaction of log reviews and score, and flexible daily dummies on time to/since the NYT reviews and since release.

Findings: 
- Price elasticities of about 0.19, twice bigger than mine, but in the same surprising low range. 

-  10% increase in score (moving from 4 to 4.4 star score) increases sales by 7.8%

- They find ``the right'' signs on the interaction of reviews and score. The interaction result tells us that the score does not matter much for products with few reviews, and the effect monotonically increases with the number of reviews (they did it by deciles in addtion to the simple intereaction)

- They also have a negative coefficient on log(reviews): more reviews are associated with higher sales rank in their data (and lower sales). 

- They have done the discontinuity thing, but it is in the appendix. There are no jumps whatsoever on their pictures.},
	Author = {Reimers, Imke C and Waldfogel, Joel},
	Date-Added = {2020-07-02 15:01:11 -0400},
	Date-Modified = {2020-07-04 13:08:34 -0400},
	Institution = {National Bureau of Economic Research},
	Keywords = {Sales, Causal, Econ},
	Title = {Digitization and Pre-Purchase Information: The Causal and Welfare Impacts of Reviews and Crowd Ratings},
	Year = {2020}}

@article{ZhuZhang10,
	Abstract = {This article examines how product and consumer characteristics moderate the influence of online consumer reviews on product sales using data from the video game industry. The findings indicate that online reviews are more influential for less popular games and games whose players have greater Internet experience. The article shows differential impact of consumer reviews across products in the same product category and suggests that firms' online marketing strategies should be contingent on product and consumer characteristics. The authors discuss the implications of these results in light of the increased share of niche products in recent years.},
	Author = {Feng Zhu and Xiaoquan (Michael) Zhang},
	Date-Added = {2020-07-02 14:34:06 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Issn-Hide = {00222429},
	Journal = {Journal of Marketing},
	Keywords = {Sales, Video Games},
	Number = {2},
	Pages = {133--148},
	Publisher = {American Marketing Association},
	Title = {Impact of Online Consumer Reviews on Sales: The Moderating Role of Product and Consumer Characteristics},
	Volume = {74},
	Year = {2010},
	Bdsk-Url-1 = {http://www.jstor.org/stable/20619095}}

@unpublished{SorokinStevens20,
	Abstract = {Using high frequency data from a large video game marketplace we study the causal effect of reviews on sales. First, by restricting attention to products with unlikely quality changes and controlling for pricing behavior of firms, we make the case that panel data methods could be sufficient for identification. Second, we use a regression discontinuity approach, lately developed in the literature, to provide quasi-experimental evidence on the subject. We uncover an identification issue with this approach, and suggest a modified version, that significantly moderates the effects. We find that having better reviews increases sales by 3-7%, that the effect is stronger for games with more reviews and later in their life cycle.},
	Author = {Sorokin, Dmitry and Stevens, Ryan Louis},
	Date-Added = {2020-07-02 12:43:58 -0400},
	Date-Modified = {2020-07-02 12:49:05 -0400},
	Keywords = {Sales, Causal},
	Month = {February},
	Title = {The Impact of Online Reviews on Product Sales: Evidence From Video Games},
	Year = {2020}}

@article{ChenEtAl19,
	Abstract = {Managerial responses to online customer reviews not only affect customers who receive the responses but may also influence subsequent customers who observe the responses. This externality arises because of the public nature of online interactions. However, prior studies were mainly in offline settings where such externality rarely exists. In this study, we assess the magnitude of such externality. Using a difference-in-difference-in-differences framework and matched hotels across two large travel agencies, we find that managerial responses indeed have a significant and positive impact on the volume of subsequent customer reviews. The impact on the review valence is not evident, which can be attributed to the unique design of identity disclosure in our research context. Furthermore, our results suggest nuances that were not known in the prior literature. For example, responding to positive and negative reviews may have different effects on future reviews, and managers should provide detailed responses to negative reviews but brief ones to positive reviews. Our results offer managerial implications to service providers on how to improve customer engagement in the interconnected online environment.},
	Author = {Chen, Wei and Gu, Bin and Ye, Qiang and Zhu, Kevin Xiaoguo},
	Date-Added = {2020-07-01 16:56:27 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/isre.2018.0781},
	Issn-Hide = {1526-5536},
	Journal = {Information Systems Research},
	Keywords = {Hotels, Responses},
	Month = {Mar},
	Number = {1},
	Pages = {81--96},
	Publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	Title = {Measuring and Managing the Externality of Managerial Responses to Online Customer Reviews},
	Volume = {30},
	Year = {2019},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/isre.2018.0781}}

@article{MaEtAl15,
	Abstract = {Firms are increasingly engaging with customers on social media. Despite this heightened interest, guidance for effective engagement is lacking. In this study, we investigate customers' compliments and complaints and firms' service interventions on social media. We develop a dynamic choice model that explicitly accounts for the evolutions of both customers' voicing decisions and their relationships with the firm. Voices are driven by both the customers' underlying relationships and other factors such as redress seeking. We estimate the model using a unique data set of customer voices and service interventions on Twitter. We find that redress seeking is a major driver of customer complaints, and although service intervention improves relationships, it also encourages more complaints later. Because of this dual effect, firms are likely to underestimate the returns on service intervention if measured using only voices. Furthermore, we find an "error-correction" effect in certain situations, where customers compliment or complain when others voice the opposite opinions. Finally, we characterize the distinct voicing tendencies in different relationship states, and show that uncovering the underlying relationship states enables effective targeting. We are among the first to analyze individual customer level voice dynamics and to evaluate the effects of service intervention on social media.},
	Annote = {Chevalier et. al 18 and this paper belong to the ``nascent literature on the impact of managerial response on consumer voice.'' Both papers study the feedback loop of firms responding to reviews, and how that changes the behavior of reviewers. Unlike their settings, discounts are not personalized, so should not have any feedback loops like that.},
	Author = {Ma, Liye and Sun, Baohong and Kekre, Sunder},
	Date-Added = {2020-07-01 16:38:24 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/mksc.2015.0912},
	Issn-Hide = {1526-548X},
	Journal = {Marketing Science},
	Keywords = {Responses},
	Month = {Sep},
	Number = {5},
	Pages = {627--645},
	Publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	Title = {The Squeaky Wheel Gets the Grease---An Empirical Analysis of Customer Voice and Firm Intervention on Twitter},
	Volume = {34},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/mksc.2015.0912}}

@article{ChevalierEtAl18,
	Abstract = {We examine the effect of managerial response on consumer voice in a dynamic quality environment. We argue that the consumer is motivated to write reviews not only because reviews may impact other consumers, but because reviews may impact the management and the quality of the service. We examine this empirically in a scenario in which reviewers receive a credible signal that the service provider is listening. Specifically, we examine the ``managerial response'' feature allowed by many review platforms. We hypothesize that managerial responses will stimulate reviewing activity and, in particular, will stimulate negative reviews that are seen as more impactful. This effect is further heightened because managers respond more and in more detail to negative reviews. Using a multiple-differences specification, we show that reviewing activity and particularly negative reviewing is indeed stimulated by managerial response. Our specification exploits comparison of the same hotel immediately before and after response initiation and compares a given hotel's reviewing activity on sites with review response initiation to that on sites that do not allow managerial response. We also explore the mechanism behind the effect using an online experiment.},
	Annote = {A quote from the paper: ``While our setting is the same as that of Proserpio and Zervas (2017), our paper differs from theirs both methodologically and in the data that we use. Our result that managerial response leads to a decrease in review valence starkly contrasts with their result that review valence increases following the initiation of managerial response.''

In Section 9 they elaborate extensively on the difference between their paper and Proserpio and Zervas (2017) and Ma et Al 15. 

This paper has a good lit review for hotels and responses.},
	Author = {Chevalier, Judith A. and Dover, Yaniv and Mayzlin, Dina},
	Date-Added = {2020-07-01 16:27:35 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/mksc.2018.1090},
	Issn-Hide = {1526-548X},
	Journal = {Marketing Science},
	Keywords = {Responses, Hotels, Theory},
	Month = {Sep},
	Number = {5},
	Pages = {688--709},
	Publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	Title = {Channels of Impact: User Reviews When Quality Is Dynamic and Managers Respond},
	Volume = {37},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/mksc.2018.1090}}

@article{GuYe14,
	Abstract = {With the growing influence of online social media, firms increasingly take an active role in interacting with consumers in social media. For many firms, their first step in online social media is management responses, where the management responds to customers' comments about the firm or its products and services. In this article, we measure the impact of management responses on customer satisfaction using data retrieved from a major online travel agency in China. Applying a panel data model that controls for regression toward the mean and heterogeneity in individual preference for hotels, we find that online management responses are highly effective among low satisfaction customers but have limited influence on other customers. Moreover, we show that the public nature of online management responses introduces a new dynamic among customers. Although online management responses increase future satisfaction of the complaining customers who receive the responses, they decrease future satisfaction of complaining customers who observe but do not receive management responses. The result is consistent with the peer-induced fairness theory.},
	Annote = {Using data on customers who have reviewed a hotel more than once in China, they show that consumers who left bad reviews in the past are more likely to leave a good review in the future if their previous reviews was responded to, controlling for ``regression to the mean''.},
	Author = {Gu, Bin and Ye, Qiang},
	Date-Added = {2020-07-01 15:44:09 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1111/poms.12043},
	Eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/poms.12043},
	Journal = {Production and Operations Management},
	Keywords = {Responses, Hotels},
	Number = {4},
	Pages = {570-582},
	Title = {First Step in Social Media: Measuring the Influence of Online Management Responses on Customer Satisfaction},
	Volume = {23},
	Year = {2014},
	Bdsk-Url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/poms.12043},
	Bdsk-Url-2 = {https://doi.org/10.1111/poms.12043}}

@article{XieEtAl17,
	Abstract = {Hotels are increasingly shifting their online review strategy from passive listening to proactive engagement through management responses. This study investigates the joint effects of management responses and online reviews on hotel financial performance. Based on a large unique dataset of 22,483 management responses to 76,649 online consumer reviews on TripAdvisor over 26 quarters, matched with quarterly hotel financial performance, this study finds that providing timely and lengthy responses enhances future financial performance, whereas providing responses by hotel executives and responses that simply repeat topics in the online review lowers future financial performance. Moreover, review rating and review volume moderate the effects of management responses. When the average review ratings increase, more management responses of greater length should be provided. As review volume grows, the benefits of providing timely and lengthy responses diminish. The study findings generate new implications for managing responses to online reviews to increase hotel financial performance.},
	Annote = {This paper finds that managerial responses are associated with better financial performance in the following quarter.},
	Author = {Karen L. Xie and Kevin Kam Fung So and Wei Wang},
	Date-Added = {2020-07-01 15:35:11 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {https://doi.org/10.1016/j.ijhm.2016.12.004},
	Issn-Hide = {0278-4319},
	Journal = {International Journal of Hospitality Management},
	Keywords = {Responses, Hotels, Sales},
	Pages = {101 - 110},
	Title = {Joint effects of management responses and online reviews on hotel financial performance: A data-analytics approach},
	Volume = {62},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0278431916305151},
	Bdsk-Url-2 = {https://doi.org/10.1016/j.ijhm.2016.12.004}}

@article{XieEtAl16,
	Abstract = {Purpose
This study aims to measures the effects of managerial response on consumer electronic word-of-mouth (eWOM) and hotel performance.

Design/methodology/approach
A sample of 56,284 consumer reviews and 10,793 managerial responses for 1,045 hotels was retrieved from TripAdvisor, along with 30,232 performance records matched to these hotels on a quarterly basis.

Findings
This study finds that managerial response leads to an average increase of 0.235 stars in the TripAdvisor ratings of the sampled hotels, as well as a 17.3 per cent increase in the volume of subsequent consumer eWOM. Moreover, managerial response moderates the influence of ratings and volume of consumer eWOM on hotel performance.

Practical implications
This study offers a practical model that enables hotel managers to orchestrate social media marketing approaches and efforts toward an optimal social media strategy.

Originality/value
This study differs from extant literature that has extensively focused on consumer reviews by providing a new perspective of management intervention in the social media context. By examining the interplay of managerial response and consumer eWOM at the individual hotel level, this study provides empirical evidence of managerial response affecting hotel performance through the increased ratings and volume of consumer eWOM. This study also offers insights into the practical importance of crafting intervention opportunities to cultivate the continued engagement of consumers on social media and increased hotel performance.},
	Annote = {Looks whatever, similar to Proserpio and Zervas, not sure what the difference is.},
	Author = {Xie, Karen L. and Zhang, Zili and Zhang, Ziqiong and Singh, Amrik and Lee, Seul Ki},
	Date-Added = {2020-07-01 15:23:25 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1108/ijchm-06-2015-0290},
	Issn-Hide = {0959-6119},
	Journal = {International Journal of Contemporary Hospitality Management},
	Keywords = {Manipulation, Responses, Hotels},
	Month = {Sep},
	Number = {9},
	Pages = {2013--2034},
	Publisher = {Emerald},
	Title = {Effects of managerial response on consumer eWOM and hotel performance},
	Volume = {28},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1108/IJCHM-06-2015-0290},
	Bdsk-Url-2 = {http://dx.doi.org/10.1108/ijchm-06-2015-0290}}

@article{ProserpioZervas17,
	Abstract = {We investigate the relationship between a firm's use of management responses and its online reputation. We focus on the hotel industry and present several findings. First, hotels are likely to start responding following a negative shock to their ratings. Second, hotels respond to positive, negative, and neutral reviews at roughly the same rate. Third, by exploiting variation in the rate with which hotels respond on different review platforms and variation in the likelihood with which consumers are exposed to management responses, we find a 0.12-star increase in ratings and a 12% increase in review volume for responding hotels. Interestingly, when hotels start responding, they receive fewer but longer negative reviews. To explain this finding, we argue that unsatisfied consumers become less likely to leave short indefensible reviews when hotels are likely to scrutinize them. Our results highlight an interesting trade-off for managers considering responding: fewer negative ratings at the cost of longer and more detailed negative feedback.},
	Annote = {Really well-written intro, maybe I should conslut is while writing.

The document that managerial responses follow closely a dip in reviews, akin to Ashenfelter's dip. Maybe I should speak to that, and plot average reviews around discounts? Maybe I can find a dip as well.

It is key that they only study responses in the form of managers responding to reviews. My focus is on prices, which is arguably bigger.},
	Author = {Proserpio, Davide and Zervas, Georgios},
	Date-Added = {2020-07-01 14:49:30 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/mksc.2017.1043},
	Issn-Hide = {1526-548X},
	Journal = {Marketing Science},
	Keywords = {Responses, Manipulation},
	Month = {Sep},
	Number = {5},
	Pages = {645--665},
	Publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	Title = {Online Reputation Management: Estimating the Impact of Management Responses on Consumer Reviews},
	Volume = {36},
	Year = {2017},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/mksc.2017.1043}}

@article{ZervasEtAl15,
	Abstract = {Judging by the millions of reviews left by guests on the Airbnb platform, this "trusted community marketplace" is fulfilling its mission of matching travelers seeking accommodation with hosts who have room to spare remarkably well. Based on our analysis of ratings we collected for over 600,000 properties listed on Airbnb worldwide, we find that nearly 95% of Airbnb properties boast an average user-generated rating of either 4.5 or 5 stars (the maximum); virtually none have less than a 3.5 star rating. We contrast this with the ratings of approximately half a million hotels worldwide that we collected on TripAdvisor, where there is a much lower average rating of 3.8 stars, and more variance across reviews. Considering properties by accommodation type and by location, we find considerable variability in ratings, and observe that vacation rental properties on TripAdvisor have ratings most similar to ratings of Airbnb properties. Last, we consider several thousand properties that are listed on both platforms. For these cross-listed properties, we find that even though the average ratings on Airbnb and TripAdvisor are similar, proportionally more properties receive the highest ratings (4.5 stars and above) on Airbnb than on TripAdvisor. Moreover, there is only weak correlation in the ratings of individual cross-listed properties across the two platforms. Our work is a first step towards understanding and interpreting nuances of user-generated ratings in the context of the sharing economy.},
	Annote = {Can refer to them when talking about the distribution of review scores},
	Author = {Zervas, Georgios and Proserpio, Davide and Byers, John},
	Date-Added = {2020-06-30 15:35:55 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.2139/ssrn.2554500},
	Issn-Hide = {1556-5068},
	Journal = {SSRN Electronic Journal},
	Keywords = {Inflation},
	Publisher = {Elsevier BV},
	Title = {A First Look at Online Reputation on Airbnb, Where Every Stay is Above Average},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.2139/ssrn.2554500}}

@article{Sahoo18,
	Abstract = {Although many researchers in information systems and marketing have studied the effect of product reviews on sales, few have looked at their effect on product returns. We hypothesize that, by reducing product uncertainty, product reviews affect the probability of product returns. We elaborate this hypothesis starting with an analytical model that examines how changes in valence and precision of information from product reviews influence the purchase and return probabilities of risk-averse, but rational, consumers. We then empirically test our hypotheses using a transaction-level data set from a multichannel, multibrand North American specialty retailer. Harnessing different consumers' purchases and returns of the same products, but with varying sets of product reviews over two years, we show that the availability of more reviews and the presence of more ``helpful'' reviews, as voted by consumers, lead to fewer product returns---after controlling for customer, product, and other context-related factors. Analyzing the purchase behavior of the consumers, we find that when fewer product reviews are available, consumers buy more substitutes in conjunction with a product, potentially to mitigate their uncertainty. Purchase of substitutes, in turn, leads to more product returns. Finally, leveraging a discontinuity in the displayed average ratings, we find that when products are shown with an average rating that is higher than the true rating, they are returned more often. These results support the predictions of our theoretical model---unbiased online reviews indeed help consumers make better purchase decisions, leading to lower product returns; biasing reviews upward results in more returns. The presence of online reviews has important cost implications for the firm beyond the cost of reprocessing the returns; we observe that when consumers return products, they are more likely to write online reviews and that these reviews are more negative than reviews that follow a nonreturned purchase.},
	Author = {Sahoo, Nachiketa and Dellarocas, Chrysanthos and Srinivasan, Shuba},
	Date-Added = {2020-06-30 15:25:29 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/isre.2017.0736},
	Issn-Hide = {1526-5536},
	Journal = {Information Systems Research},
	Keywords = {Sales},
	Month = {Sep},
	Number = {3},
	Pages = {723--738},
	Publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	Title = {The Impact of Online Product Reviews on Product Returns},
	Volume = {29},
	Year = {2018},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/isre.2017.0736}}

@article{ChevalierMayzlin06,
	Abstract = {The authors examine the effect of consumer reviews on relative sales of books at Amazon.com and Barnesandnoble.com. The authors find that (1) reviews are overwhelmingly positive at both sites, but there are more reviews and longer reviews at Amazon.com; (2) an improvement in a book's reviews leads to an increase in relative sales at that site; (3) for most samples in the study, the impact of one-star reviews is greater than the impact of five-star reviews; and (4) evidence from review-length data suggests that customers read review text rather than relying only on summary statistics.},
	Author = {Chevalier, Judith A. and Mayzlin, Dina},
	Date-Added = {2020-06-30 13:39:31 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1509/jmkr.43.3.345},
	Issn-Hide = {1547-7193},
	Journal = {Journal of Marketing Research},
	Keywords = {Sales, Causal},
	Month = {Aug},
	Number = {3},
	Pages = {345--354},
	Publisher = {SAGE Publications},
	Title = {The Effect of Word of Mouth on Sales: Online Book Reviews},
	Volume = {43},
	Year = {2006},
	Bdsk-Url-1 = {http://dx.doi.org/10.1509/jmkr.43.3.345}}

@article{LucaZervas16,
	Abstract = {Consumer reviews are now part of everyday decision making. Yet the credibility of these reviews is fundamentally undermined when businesses commit review fraud, creating fake reviews for themselves or their competitors. We investigate the economic incentives to commit review fraud on the popular review platform Yelp, using two complementary approaches and data sets. We begin by analyzing restaurant reviews that are identified by Yelp's filtering algorithm as suspicious, or fake---and treat these as a proxy for review fraud (an assumption we provide evidence for). We present four main findings. First, roughly 16% of restaurant reviews on Yelp are filtered. These reviews tend to be more extreme (favorable or unfavorable) than other reviews, and the prevalence of suspicious reviews has grown significantly over time. Second, a restaurant is more likely to commit review fraud when its reputation is weak, i.e., when it has few reviews or it has recently received bad reviews. Third, chain restaurants---which benefit less from Yelp---are also less likely to commit review fraud. Fourth, when restaurants face increased competition, they become more likely to receive unfavorable fake reviews. Using a separate data set, we analyze businesses that were caught soliciting fake reviews through a sting conducted by Yelp. These data support our main results and shed further light on the economic incentives behind a business's decision to leave fake reviews.},
	Author = {Luca, Michael and Zervas, Georgios},
	Date-Added = {2020-06-29 18:54:33 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/mnsc.2015.2304},
	Issn-Hide = {1526-5501},
	Journal = {Management Science},
	Keywords = {Manipulation},
	Month = {Dec},
	Number = {12},
	Pages = {3412--3427},
	Publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	Title = {Fake It Till You Make It: Reputation, Competition, and Yelp Review Fraud},
	Volume = {62},
	Year = {2016},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/mnsc.2015.2304}}

@article{DellarocasWood08,
	Abstract = { Most online feedback mechanisms rely on voluntary reporting of privately observed outcomes. This introduces the potential for reporting bias, a situation where traders exhibit different propensities to report different outcome types to the system. Unless properly accounted for, reporting bias may severely distort the distribution of public feedback relative to the underlying distribution of private transaction outcomes and, thus, hamper the reliability of feedback mechanisms. This study offers a method that allows users of feedback mechanisms where both partners of a bilateral exchange are allowed to report their satisfaction to ``see through'' the distortions introduced by reporting bias and derive unbiased estimates of the underlying distribution of privately observed outcomes. A key aspect of our method lies in extracting information from the number of transactions where one or both trading partners choose to remain silent. We apply our method to a large data set of eBay feedback. Our results support the widespread belief that eBay traders are more likely to post feedback when satisfied than when dissatisfied and are consistent with the presence of positive and negative reciprocation among eBay traders. Most importantly, our analysis derives unbiased estimates of the risks that are associated with trading on eBay that, we believe, are more realistic than those suggested by a na{\"\i}ve interpretation of the unusually high (>99\%) levels of positive feedback currently found on that system. },
	Author = {Dellarocas, Chrysanthos and Wood, Charles A.},
	Date-Added = {2020-06-29 18:50:06 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/mnsc.1070.0747},
	Eprint = {https://pubsonline.informs.org/doi/pdf/10.1287/mnsc.1070.0747},
	Journal = {Management Science},
	Keywords = {Theory, Selection},
	Number = {3},
	Pages = {460-476},
	Title = {The Sound of Silence in Online Feedback: Estimating Trading Risks in the Presence of Reporting Bias},
	Volume = {54},
	Year = {2008},
	Bdsk-Url-1 = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.1070.0747},
	Bdsk-Url-2 = {https://doi.org/10.1287/mnsc.1070.0747}}

@article{LiHitt08,
	Abstract = {Online product reviews may be subject to self-selection biases that impact consumer purchase behavior, online ratings' time series, and consumer surplus. This occurs if early buyers hold different preferences than do later consumers about the quality of a given product. Readers of early product reviews may not successfully correct for these preference differences when interpreting ratings and making purchases. In this study, we develop a model that examines how idiosyncratic preferences of early buyers can affect long-term consumer purchase behavior as well as the social welfare created by review systems. Our model provides an explanation for the structure of product ratings over time, which we empirically test using online book reviews posted on Amazon.com. Our analysis suggests that firms could benefit from altering their marketing strategies such as pricing, advertising, or product design to encourage consumers likely to yield positive reports to self-select into the market early and generate positive word-of-mouth for new products. On the other hand, self-selection bias, if not corrected, decreases consumer surplus.},
	Author = {Li, Xinxin and Hitt, Lorin},
	Date-Added = {2020-06-29 18:44:47 -0400},
	Date-Modified = {2020-06-29 18:45:09 -0400},
	Doi-Hide = {10.1287/isre.1070.0154},
	Journal = {Information Systems Research},
	Keywords = {Theory, Selection},
	Month = {12},
	Pages = {456-474},
	Title = {Self Selection and Information Role of Online Product Reviews},
	Volume = {19},
	Year = {2008},
	Bdsk-Url-1 = {https://doi.org/10.1287/isre.1070.0154}}

@article{Berger10,
	Abstract = {Can negative information about a product increase sales, and if so, when? Although popular wisdom suggests that ``any publicity is good publicity,'' prior research has demonstrated only downsides to negative press. Negative reviews or word of mouth, for example, have been found to hurt product evaluation and sales. Using a combination of econometric analysis and experimental methods, we unify these perspectives to delineate contexts under which negative publicity about a product will have positive versus negative effects. Specifically, we argue that negative publicity can increase purchase likelihood and sales by increasing product awareness. Consequently, negative publicity should have differential effects on established versus unknown products. Three studies support this perspective. Whereas a negative review in the New York Times hurt sales of books by well-known authors, for example, it increased sales of books that had lower prior awareness. The studies further underscore the importance of a gap between publicity and purchase occasion and the mediating role of increased awareness in these effects.},
	Annote = {They rely on the timing of the NYT review to be orthogonal to unobserved shocks, which is a bit simialr to how I think about the endogeneity of prices.},
	Author = {Berger, Jonah and Sorensen, Alan T. and Rasmussen, Scott J.},
	Date-Added = {2020-06-29 18:37:29 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1287/mksc.1090.0557},
	Issn-Hide = {1526-548X},
	Journal = {Marketing Science},
	Keywords = {Sales},
	Month = {Sep},
	Number = {5},
	Pages = {815--827},
	Publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
	Title = {Positive Effects of Negative Publicity: When Negative Reviews Increase Sales},
	Volume = {29},
	Year = {2010},
	Bdsk-Url-1 = {http://dx.doi.org/10.1287/mksc.1090.0557}}

@article{Mayzlin14,
	Abstract = {Firms' incentives to manufacture biased user reviews impede review usefulness. We examine the differences in reviews for a given hotel between two sites: Expedia.com (only a customer can post a review) and TripAdvisor.com (anyone can post). We argue that the net gains from promotional reviewing are highest for independent hotels with single-unit owners and lowest for branded chain hotels with multi-unit owners. We demonstrate that the hotel neighbors of hotels with a high incentive to fake have more negative reviews on TripAdvisor relative to Expedia; hotels with a high incentive to fake have more positive reviews on TripAdvisor relative to Expedia.},
	Annote = {Shows that firms respond to incentives and post fake reviews when it benefits them. Several implications for me. First, doing something to manipulate your review score is present in other industries. Second, they find that (smaller) independent hotels are more likely to have fake positive reviews, compared to chains. In my world this means that independent developers might be more likely to leave good fake reviews for themselves. Third, Steam only allows actual buyers to leave reviews, so Steam is more like Expedia in their paper (and not Trip Advisor, where everybody can leave reviews). Their finding that competitors leave bad reviews to each other might or might not hold on Steam, as the number of competitors is probably bigger, but who knows (the paper looks at neighbors within a small distance away from the hotel, while distance in the product space is harder to quantify).},
	Author = {Mayzlin, Dina and Dover, Yaniv and Chevalier, Judith},
	Date-Added = {2020-06-29 17:24:47 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1257/aer.104.8.2421},
	Journal = {American Economic Review},
	Keywords = {Manipulation},
	Month = {August},
	Number = {8},
	Pages = {2421-55},
	Title = {Promotional Reviews: An Empirical Investigation of Online Review Manipulation},
	Volume = {104},
	Year = {2014},
	Bdsk-Url-1 = {https://www.aeaweb.org/articles?id=10.1257/aer.104.8.2421},
	Bdsk-Url-2 = {https://doi.org/10.1257/aer.104.8.2421}}

@webpage{Harmon04,
	Annote = {A NYT article that I found in Mayzlin's AER piece. Publishers and authors were caught leaving fake reviews for their books on Amazon.},
	Author = {Harmon, Amy},
	Date-Added = {2020-06-29 17:02:57 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Journal = {The New York Times},
	Keywords = {News, Manipulation},
	Month = {February},
	Title = {Amazon Glitch Unmasks War Of Reviewers},
	Year = {2004},
	Bdsk-Url-1 = {https://www.nytimes.com/2004/02/14/us/amazon-glitch-unmasks-war-of-reviewers.html}}

@article{Resnick03,
	Abstract = {We conducted the first randomized controlled study of an Internet reputation mechanism. A high-reputation, established eBay dealer sold matched pairs of items -- batches of vintage postcards -- under his regular identity and new seller identities (also operated by him). As predicted, the established identity fared better. The difference in buyers' willingness-to-pay was 8.1% of the selling price. A subsidiary experiment followed the same format, but compared sales by relatively new sellers with and without negative feedback. Surprisingly, one or two negative feedbacks for our new sellers did not affect buyers' willingness-to-pay. },
	Annote = {Consumers' WTP was found to be 8.1% higher for a vendor with established reputation selling matched objects of similar quality.},
	Author = {Resnick, Paul and Zeckhauser, Richard and Swanson, John and Lockwood, Kate},
	Date-Added = {2020-06-29 16:43:44 -0400},
	Date-Modified = {2020-06-29 16:44:48 -0400},
	Doi-Hide = {10.2139/ssrn.385206},
	Journal = {Experimental Economics},
	Keywords = {Experiment, Reputation},
	Month = {07},
	Title = {The Value Of Reputation On Ebay: A Controlled Experiment},
	Volume = {9},
	Year = {2003},
	Bdsk-Url-1 = {https://doi.org/10.2139/ssrn.385206}}

@article{Floyd14,
	Abstract = {A growing body of research has emerged on online product reviews and their ability to elicit performance outcomes desired by retailers; yet, a common understanding of the performance implications of online product reviews has eluded us. Scholars continue to navigate an array of studies assessing different design elements of online product reviews, and various research settings and data sources. We undertake a meta-analysis of 26 empirical studies yielding 443 sales elasticities to examine how these variables relate to retail sales. Building on well-established meta-analytical methods, we address the following questions: How does review valence influence the elasticity of retailer sales? What about review volume? For which product types and usage situations do online product reviews have a greater impact on retailer sales elasticity? Which types of online reviewers and websites exert the greatest influence on retailer sales elasticity? Our study answers these important questions and provides a much needed quantitative synthesis of this burgeoning stream of research.},
	Annote = {Lit review},
	Author = {Floyd, Kristopher and Freling, Ryan and Alhoqail, Saad and Cho, Hyun Young and Freling, Traci},
	Date-Added = {2020-06-29 16:32:27 -0400},
	Date-Modified = {2020-06-29 16:33:13 -0400},
	Doi-Hide = {10.1016/j.jretai.2014.04.004},
	Journal = {Journal of Retailing},
	Keywords = {Meta, Literature Review},
	Month = {06},
	Title = {How Online Product Reviews Affect Retail Sales: A Meta-analysis},
	Volume = {90},
	Year = {2014},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.jretai.2014.04.004}}

@article{deLanghe15,
	Abstract = {This research documents a substantial disconnect between the objective quality information that online user ratings actually convey and the extent to which consumers trust them as indicators of objective quality. Analyses of a data set covering 1272 products across 120 vertically differentiated product categories reveal that average user ratings (1) lack convergence with Consumer Reports scores, the most commonly used measure of objective quality in the consumer behavior literature, (2) are often based on insufficient sample sizes which limits their informativeness, (3) do not predict resale prices in the used-product marketplace, and (4) are higher for more expensive products and premium brands, controlling for Consumer Reports scores. However, when forming quality inferences and purchase intentions, consumers heavily weight the average rating compared to other cues for quality like price and the number of ratings. They also fail to moderate their reliance on the average user rating as a function of sample size sufficiency. Consumers' trust in the average user rating as a cue for objective quality appears to be based on an ``illusion of validity.''},
	Annote = {Consumers seem to put a lot of weight on ``Stars''. I can say that reviews are widely used. This could be used to motivate regressing things on review bin dummies. },
	Author = {de Langhe, Bart and Fernbach, Philip M. and Lichtenstein, Donald R.},
	Date-Added = {2020-06-29 16:10:16 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Doi-Hide = {10.1093/jcr/ucv047},
	Issn-Hide = {0093-5301},
	Journal = {Journal of Consumer Research},
	Keywords = {Reviews, Biases},
	Month = {09},
	Number = {6},
	Pages = {817-833},
	Title = {Navigating by the Stars: Investigating the Actual and Perceived Validity of Online User Ratings},
	Volume = {42},
	Year = {2015},
	Bdsk-Url-1 = {https://doi.org/10.1093/jcr/ucv047}}

@unpublished{Filippas20,
	Abstract = {A solution to marketplace information asymmetries is to have trading partners publicly
rate each other post-transaction. Many have shown that these ratings are effective; we
show that their effectiveness can deteriorate over time. The problem is that ratings are
prone to inflation, with raters feeling pressure to leave ``above average'' ratings, which
in turn pushes the average higher. This pressure stems from raters' desire to not harm
the rated seller. As the potential to harm is what makes ratings effective, reputation
systems, as typically designed, become less informative in the long-run.},
	Annote = {I can refer to this paper when showing the distribution of scores on Steam (very skewed towards good reviews scores). From the perspective of review design, Steam's review system, which only gives the user 2 reviews options---like and dislike---could be less prone to the inflation of reviews because of the reasons listed in the paper.},
	Author = {Filippas, Apostolos and Horton, John J. and Golden, Joseph M.},
	Date-Added = {2020-06-29 15:24:42 -0400},
	Date-Modified = {2020-10-13 11:37:08 -0400},
	Keywords = {Reviews, Inflation, Biases},
	Title = {Reputation Inflation},
	Year = {2020},
	Bdsk-Url-1 = {http://john-joseph-horton.com/papers/longrun.pdf}}
